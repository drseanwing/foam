# =============================================================================
# FOAM Workflow Ollama Model Configuration
# =============================================================================
# This file documents the Ollama models used in the FOAM workflow
# Run: cat ollama-models.txt | xargs -I {} ollama pull {}
# to pull all required models

# =============================================================================
# PRIMARY MODELS
# =============================================================================

# Llama 3.2 - Primary local model for preprocessing and summarisation
# Good balance of speed and capability
# Size: ~4GB
llama3.2:latest

# Mistral - Fallback model, good for validation tasks
# Size: ~4GB
mistral:latest

# =============================================================================
# OPTIONAL MODELS
# =============================================================================

# Llama 3.2 Vision - If processing images/charts from papers
# Size: ~5GB
# llama3.2-vision:latest

# CodeLlama - If code generation needed
# Size: ~4GB
# codellama:latest

# Mixtral - Higher capability, but larger
# Size: ~26GB - Requires significant VRAM
# mixtral:latest

# =============================================================================
# MODEL SELECTION GUIDE
# =============================================================================
#
# Task                     | Recommended Model      | Reason
# -------------------------|------------------------|---------------------------
# Text summarisation       | llama3.2:latest        | Fast, good compression
# Preprocessing            | llama3.2:latest        | Fast, consistent
# Checklist validation     | mistral:latest         | Good at structured tasks
# Format conversion        | llama3.2:latest        | Handles templates well
# Fallback generation      | mistral:latest         | More creative
#
# =============================================================================
# HARDWARE REQUIREMENTS
# =============================================================================
#
# Minimum for llama3.2 + mistral:
# - 16GB RAM
# - 8GB VRAM (NVIDIA GPU recommended)
# - 20GB disk space
#
# Recommended for smooth operation:
# - 32GB RAM
# - 16GB VRAM
# - 50GB disk space
#
# =============================================================================
# INSTALLATION
# =============================================================================
#
# 1. Install Ollama: https://ollama.ai/download
#
# 2. Pull models:
#    ollama pull llama3.2:latest
#    ollama pull mistral:latest
#
# 3. Verify installation:
#    ollama list
#
# 4. Test models:
#    ollama run llama3.2 "Hello, test message"
#
# =============================================================================
# DOCKER NOTES
# =============================================================================
#
# When running Ollama in Docker alongside N8N:
#
# 1. Use host.docker.internal for connectivity:
#    OLLAMA_BASE_URL=http://host.docker.internal:11434
#
# 2. On Linux, add to docker run:
#    --add-host=host.docker.internal:host-gateway
#
# 3. For GPU support, add:
#    --gpus all
#
# =============================================================================
# N8N CONFIGURATION NOTES
# =============================================================================
#
# CRITICAL: In N8N, use "Ollama Chat Model" node, NOT "Ollama Model"
# Only the Chat variant supports tool calling required by AI Agent nodes.
#
# Node configuration example:
# {
#   "parameters": {
#     "model": "llama3.2:latest",
#     "baseUrl": "http://host.docker.internal:11434"
#   },
#   "type": "@n8n/n8n-nodes-langchain.lmChatOllama"
# }
#
